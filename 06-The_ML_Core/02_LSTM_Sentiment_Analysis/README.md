# üé¨ LSTM Sentiment Analysis

**Project 2 of the ML Core - AI Engineer Roadmap**

A lightweight LSTM neural network for sentiment analysis on movie reviews, optimized for CPU training.

---

## üìã Overview

This project implements a sentiment classification system using LSTM networks to analyze movie reviews from the IMDb dataset. The model is heavily optimized to run efficiently on older CPUs while maintaining good accuracy.

**Key Features:**
- ‚úÖ Fast simple tokenization (no heavy NLP libraries during training)
- ‚úÖ Optimized for low-end hardware (old CPUs)
- ‚úÖ Dynamic batch size handling
- ‚úÖ Achieved 98% validation accuracy on test subset
- ‚úÖ Complete end-to-end pipeline (preprocessing ‚Üí training ‚Üí inference)

**Technologies:**
- PyTorch (Deep Learning)
- Pandas (Data Processing)
- NumPy (Numerical Computing)

---

## üìÅ Project Structure

```
02_LSTM_Sentiment_Analysis/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py              # LSTM model architecture
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py         # Fast text preprocessing & tokenization
‚îÇ   ‚îî‚îÄ‚îÄ train.py              # Training script with optimizations
‚îÇ
‚îú‚îÄ‚îÄ data/                      # Dataset directory (gitignored - too large)
‚îÇ   ‚îú‚îÄ‚îÄ imdb_train.csv        # Training data (download required)
‚îÇ   ‚îú‚îÄ‚îÄ imdb_test.csv         # Testing data (download required)
‚îÇ   ‚îî‚îÄ‚îÄ word2idx.json         # Vocabulary (generated during training)
‚îÇ
‚îú‚îÄ‚îÄ models/                    # Saved models (gitignored - too large)
‚îÇ   ‚îî‚îÄ‚îÄ lstm_sentiment_model.pth
‚îÇ
‚îú‚îÄ‚îÄ run_training.sh           # Quick training script
‚îú‚îÄ‚îÄ check_progress.sh         # Monitor training progress
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ OPTIMIZATIONS.md          # CPU optimization details
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### 1. Download the Dataset

Download the IMDb dataset and place it in the `data/` folder:
- [IMDb Dataset on Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

Or use the dataset loader in the code (downloads automatically).

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

**Requirements:**
```
torch>=2.0.0
numpy>=1.24.0
pandas>=2.0.0
```

### 3. Train the Model

```bash
./run_training.sh
```

Or manually:
```bash
cd 06-The_ML_Core/02_LSTM_Sentiment_Analysis
python src/train.py
```

### 4. Monitor Progress

```bash
./check_progress.sh
```

---

## üèóÔ∏è Model Architecture

### SentimentLSTM

```python
SentimentLSTM(
  (embedding): Embedding(251639, 128)      # Word embeddings
  (lstm): LSTM(128, 64, batch_first=True)  # LSTM layer
  (dropout): Dropout(p=0.5)                # Regularization
  (fc): Linear(64, 1)                      # Output layer
  (sig): Sigmoid()                         # Activation
)
```

**Optimizations for Old CPUs:**
- Embedding Dimension: 128 (reduced from 400)
- Hidden Units: 64 (reduced from 256)
- LSTM Layers: 1 (reduced from 2)
- Training Samples: 200 (for quick demo)
- Sequence Length: 128 tokens

---

## üìä Results

### Training Performance

| Metric | Value |
|--------|-------|
| **Training Loss** | 0.6174 |
| **Validation Loss** | 0.5988 |
| **Validation Accuracy** | **98.00%** |
| **Epochs** | 1 |
| **Training Time** | ~5 minutes (on old CPU) |

**Note:** Training on full dataset (25,000 samples) will yield better results but takes longer.

---

## üí° Key Optimizations

### For Old/Slow CPUs:

1. **Fast Tokenization**: Replaced NLTK's `word_tokenize()` with simple `.split()` ‚Üí **100x faster**
2. **Small Dataset**: Use subset (200 train, 100 test) for quick demo
3. **Lightweight Model**: Reduced embedding & hidden dimensions
4. **Dynamic Batching**: Handles variable batch sizes automatically
5. **No Heavy Dependencies**: Removed unnecessary libraries during training

See [OPTIMIZATIONS.md](OPTIMIZATIONS.md) for detailed changes.

---

## üéì Lessons Learned

1. **Preprocessing is Critical**: Tokenization can be the bottleneck on old hardware
2. **Batch Size Matters**: Always handle variable batch sizes in LSTM training
3. **Model Size vs Accuracy**: Smaller models can still achieve good results
4. **CPU Optimization**: Simple changes (like tokenization) make huge performance differences
5. **Sequential Data**: LSTMs are powerful for understanding context in text

---

## üîÆ Future Improvements

- [ ] Add inference script for testing on custom reviews
- [ ] Implement GRU as an alternative to LSTM
- [ ] Add attention mechanism for better performance
- [ ] Create web interface for live predictions
- [ ] Fine-tune on full dataset for production use
- [ ] Add model evaluation metrics (precision, recall, F1)
- [ ] Implement bidirectional LSTM

---

## üìù Usage Example

```python
from src.model import SentimentLSTM
from src.preprocess import preprocess_data
import torch

# Load the trained model
checkpoint = torch.load('models/lstm_sentiment_model.pth')
model = SentimentLSTM(
    checkpoint['vocab_size'],
    checkpoint['output_size'],
    checkpoint['embedding_dim'],
    checkpoint['hidden_dim'],
    checkpoint['n_layers']
)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Use for predictions
# (Add inference code here)
```

---

## ü§ù Contributing

This is a learning project. Feel free to fork and experiment!

---

## üìö References

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [PyTorch LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)
- [IMDb Dataset Paper](https://ai.stanford.edu/~amaas/data/sentiment/)

---

## üìÑ License

This project is part of the AI Engineer Roadmap learning journey.

---

**Built with ‚ù§Ô∏è for learning Deep Learning and NLP**
